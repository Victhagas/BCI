{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CyberLab Challenge",
      "provenance": [],
      "authorship_tag": "ABX9TyO/YyRHTxS/I7/9jZtSUpUY",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Victhagas/BCI/blob/main/CyberLab_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsI4kWaqHGVP"
      },
      "source": [
        "# Desafio! Criar um classificador que consiga diferenciar os números 0 e 5\n",
        "\n",
        "!pip install -Uqq fastbook\n",
        "\n",
        "import fastbook\n",
        "import numpy             as np\n",
        "import pandas            as pd\n",
        "import tensorflow        as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from fastai.vision.all import *\n",
        "from fastbook          import *\n",
        "\n",
        "fastbook.setup_book()\n",
        "matplotlib.rc('image', cmap='Greys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7zrN9IVHnJQ"
      },
      "source": [
        "# Pegar os arquivos de imagem correspondentes aos números 0 e 5 do dataset MNIST.\n",
        "# Getting the image files corresponding to the numbers 0 and 5 from the MNIST dataset.\n",
        "\n",
        "path = untar_data(URLs.MNIST) # Como o arquivo original do Dataset é comprimido, é necessário usar o untar_data.\n",
        "Path.BASE_PATH = path         # As the original Dataset file is compressed, it is necessary to use untar_data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RoBK6eYNJl3Z"
      },
      "source": [
        "path.ls() # Usar o .ls para ver os dados contidos no dataset MNIST.\n",
        "          # Using .ls to see the data from the MNIST dataset."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rThDgO7cGJW"
      },
      "source": [
        "(path/'training/0').ls() # Especificando o numeral da busca no .ls\n",
        "                         # Specifying the numeral which we are searching for, in .ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkVMN1P6LY7a"
      },
      "source": [
        "zeroes_train = (path/'training/0').ls() # Armazenando os dados de treino correspondentes ao numeral em uma variável (i.e: zero)\n",
        "fives_train  = (path/'training/5').ls() # Storing the training data corresponding to the numeral in a variable (i.e: five)\n",
        "\n",
        "zeroes_test = (path/'testing/0').ls()   # Mesmo caso das variáveis acima, porém com os dados de teste\n",
        "fives_test  = (path/'testing/5').ls()   # Same thing as above, but using the variable to save the testing data "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFgmJ69UQZ--"
      },
      "source": [
        "train_0 = torch.stack([tensor(Image.open(i)) for i in zeroes_train]).float()/255 # list comprehension para transformar todas as imagens de treino em \n",
        "# tensores. Após isso, juntar todos os novos tensores em um só (torch.stack). Por fim, deixar cada pixel desse novo tensor em escala 0~1. \n",
        "\n",
        "train_5 = torch.stack([tensor(Image.open(i)) for i in fives_train ]).float()/255 # list comprehension to transform all training images into\n",
        "# tensors. After that, join all the new tensors into one (torch.stack). Finally, let each pixel of this new tensor in 0~1 scale. \n",
        "\n",
        "test_0  = torch.stack([tensor(Image.open(i)) for i in zeroes_test ]).float()/255 # Mesma ideia da variável anterior, porém com os as imagens de teste \n",
        "\n",
        "test_5  = torch.stack([tensor(Image.open(i)) for i in fives_test  ]).float()/255 # Same idea from above, but using the testing images. \n",
        "\n",
        "print(train_0.shape, train_5.shape) # Checando os tamanhos das nossas novas variaveis! \n",
        "print(test_0.shape, test_5.shape);  # Checking the shapes of our new born variables!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgsAYL0XWkzf"
      },
      "source": [
        "train_x = torch.cat([train_0, train_5]).view(-1, 28*28) # Concatenando as duas variáveis de treino e mudando o tamanho do tensor.\n",
        "train_y = tensor([1]*len(zeroes_train) + [0]*len(fives_train)).unsqueeze(1) # Rotulando cada imagem do dataset e modelando o tensor\n",
        "\n",
        "dset = list(zip(train_x,train_y)) # Criando um dataset combinando a função zip + list, nos dados de treino.\n",
        "x,y  = dset[0]\n",
        "\n",
        "test_x    = torch.cat([test_0, test_5]).view(-1, 28*28) # Concatenating the two training variables and changing the tensor's shape.\n",
        "test_y    = tensor([1]*len(test_0) + [0]*len(test_5)).unsqueeze(1) # Labeling each image and reshaping the tensor\n",
        "\n",
        "test_dset = list(zip(test_x,test_y)) # Creating a dataset, by combining zip and list functions, in the testing data. \n",
        "\n",
        "train_x.shape,train_y.shape # Checking some of our new tensors!!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e1d6NQDYmjD"
      },
      "source": [
        "def _parameters(size, std=1.0): # The first step to initialize the weights, randomly, for every pixel.\n",
        "                                # The first step to initialize the weights, randomly, for every pixel.\n",
        "\n",
        "  return (torch.randn(size)*std).requires_grad_()\n",
        "\n",
        "def mult_mat(xn): # The fundamental equation, batch*weights + bias. With * being a matricial multiplication.\n",
        "                  # The fundamental equation, batch*weights + bias. With * being a matricial multiplication.\n",
        "  \n",
        "  return xn@weights + bias\n",
        "  \n",
        "def loss_function(predictions, targets): # Essa função vai medir a distancia entre o valor da predição do seu alvo, e limitar a predição à ser entre 0 e 1\n",
        "    predictions = predictions.sigmoid()  # This function will measure the distance of the predictions from the real values, and limit the predictions (0~1)\n",
        "    \n",
        "    return torch.where(targets==1, 1-predictions, predictions).mean() # Medindo a distancia da predição ao seu alvo.    \n",
        "                                                                      # Measuring the distance of the predictions from its target.\n",
        "\n",
        "def calc_grad(xn, yn, model): # Como o nome diz, essa função é responsavel por calcular o gradiente, e para isso, utiliza-se a loss function.\n",
        "    preds = model(xn)         # As the name says, this definition will calculate the gradient value, by using the loss function.\n",
        "    loss  = loss_function(preds, yn) \n",
        "    loss.backward()           # loss.backward() calcula a taxa de variação (dloss/dx) para cada parametro em x. \n",
        "                              # loss.backward() computes dloss/dx for every parameter x.\n",
        "\n",
        "weights = _parameters((28*28,1)) # Inicializando os Pesos (weights), que irão de fato fazer o aprendizado de máquina \n",
        "bias    = _parameters(1)         # Initializing the bias, which are going to do the machine learning.\n",
        "\n",
        "dload      = DataLoader(     dset, batch_size=256) # O DataLoades promove uma iteração entre o dataset predefinido e as batches.\n",
        "test_dload = DataLoader(test_dset, batch_size=256) # the DataLoader configurs an interation of the dataset over the batches.\n",
        "\n",
        "batch   = train_x[:4]       # Tamanho das mini-batches.\n",
        "                            # Size of the mini-batches.\n",
        "\n",
        "predict = mult_mat(batch)   # Usando a definição da função fundamental com a batch setada acima como input.\n",
        "                            # Using the definition of the fundamental function, with the batch set above as input.\n",
        "\n",
        "loss    = loss_function(predict, train_y[:4]) # Armazenando a definição loss_function já com inputs especificos em uma variável, para facilitar.\n",
        "                                              # Saving the loss_function definition in a variable, to make things easier.\n",
        "\n",
        "xn,yn   = first(dload)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IdG4b0FLmvVZ"
      },
      "source": [
        "def training_epochs(model, learn_r, _parameters): # Definição que de fato faz o treinamento, usando todos os parâmetros setados acima\n",
        "                                                  # Definition that does the training over the epochs, by using all parameters set above\n",
        "\n",
        "    for xn,yn in dload:                           \n",
        "        calc_grad(xn, yn, model)\n",
        "        for i in _parameters:\n",
        "            i.data = i.data - i.grad*learn_r # Dentro da iteração no DataLoad, ocorre outra iteração que atualiza os dados usando learning ratio.\n",
        "                                             # Inside the DataLoad for loop, occurs other iteration loop that update the data by using the learning ratio.\n",
        "            i.grad.zero_()                   \n",
        "\n",
        "def batch_acc(xn, yn):                       # Definição para corrigir e efetuar a média das predições, além de limitar o valor entre 0 e 1.\n",
        "                                             # Definition that corrects and does the mean of predictions, and fix its limits between 0 and 1.\n",
        "    preds   = xn.sigmoid()                    \n",
        "    correct = (preds>0.5) == yn\n",
        "    return correct.float().mean()\n",
        "\n",
        "def testing_epochs(model):                    # Definição para juntar a batch_acc (definição acima) para todas as batches.\n",
        "                                              # Definition that gather all batches accuracy.\n",
        "    accs = [batch_acc(model(xn), yn) for xn,yn in test_dload]\n",
        "    return round(torch.stack(accs).mean().item(), 4)\n",
        "  \n",
        "testing_epochs(mult_mat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMjNS5xGqJEI"
      },
      "source": [
        "learn_r = 1.  \n",
        "_parameters = weights,bias\n",
        "\n",
        "for i in range(30):                               # Setando a quantidade de sessões de treinamento o classificador receberá.\n",
        "                                                  # For loop to set the training sections' quantity the classifier will receive.\n",
        "  training_epochs(mult_mat,learn_r,_parameters)   \n",
        "  print(testing_epochs(mult_mat), end=' ')\n",
        "testing_epochs(mult_mat)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}